\chapter{Introduction}

There isn’t a human on this earth who can triumph over Deepmind’s AlphaGo in the game of Go, however this does not quite make AlphaGo super-intelligent in that a child may beat it at draughts. 

University of Oxford philosopher Nick Bostrom defines super-intelligence as "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest" and we are yet to develop a single AI that can adapt to our varying needs or outperform us across several tasks. Obtaining a more general intelligence requires Lifelong Learning (LL) or more simply, an AI that can remember and learn from all of its life experiences like we do, as opposed to mastering a single environment. These traits are a necessity if we want to create AI's that can master more than a single environment. 

This thesis addresses lifelong reinforcement learning (LLRL) which allows a reinforcement learning agent to generalise to more than a single task. Existing "dictionary" techniques store the necessary RL model information for each task individually with a known weakness being scalability. The topic of this thesis, \textit{Kullback-Leibler Policy Chaining} , is the preposed approach of combining existing RL techniques with aspects of Information Theory to achieve LLRL in a more scalable manner. 

The motivation behind this thesis is the idea that a RL agent could somehow \textit{remember} the important details from what it has learned in the past, without having to selectively isolate the appropriate information each time. Comparing probability distributions is fundamental to information theory and the divergence (distance) between different distributions will be used to assist our LLRL agent. We can think of the divergence between two different distributions as a difference in knowledge that an agent has between two tasks. The larger the divergence, the less common information that the tasks share. This thesis explores an approach that uses this divergence to isolate the most common pieces of knowledge from past tasks in a way that places phantom memories inside our LLRL agent's brain. 

\section{Model-Based vs Model-Free Learning}
Model-based learning is the idea an agent must fully model the world and all of its mechanics. The agent must learn what the world contains, create probability distributions over where actions will lead it, and model how the world will reward it for interacting within its environment. Model-free reinforcement learning requires an agent to learn how to optimally interact with a world in order to maximise rewards which follow strict rules. The primary difference between model-free and model-based learning, is that there is much more to learn in a model-based task. In model-free, the environment will direct the agent's state transitions based on its actions but this is not the case for model-based. In model-based, the agent much also model the probability that actions will result in transitioning to some state, as well as modelling the entire reward system. This thesis focuses not on model-based reinforcement learning, but on the less complex model-free methods. 

\section{Lifelong Learning}
Lifelong learning encompasses any field that furthers current machine learning techniques to be sufficiently capable of dealing with real world problems. For example, an AI will have to be able to learn a different task, re-learn an old task, or even re-learn multiple old tasks to be especially useful in the real world. Lifelong learning can be seen to have three primary sub areas of research: Transfer Learning (TL), Multi-task Learning (MTL), and Continuous Learning (CL). The distinction between MTL and CL in a reinforcement learning context is that multitask learning requires a complete change of environment between tasks, whereas CL addresses the re-learning of different tasks within the same environment. This thesis is relevant both the fields of TL and Continuous Reinforcement Learning (CRL) in that an agent will transfer knowledge from previous tasks to assist it the learning of both new and previously learned tasks in the same environment. 

\subsection{Issues with Current CRL Approaches}
Many recent CRL approaches adopt a dictionary approach, which involves storing all information known about a learned task. Considerable storage is therefore required if task information contains large multi-dimensional probability distributions over states and their transitions. As required by LL, a CRL agent must be useful in the real world where the number of states can grow exponentially (e.g. if we account for visual and/or audio input). If a CRL is then required to learn all of the tasks necessary to say, help an elderly person cross the street, then storing all the necessary information for every task begins to become an issue. These scalability restrictions hinder the field of CRL and limit how useful an AI can be in a practical sense. This thesis proposes a new approach that attempts to identify the essential information of a task and to reduce the storage required, while also reducing the time it takes for a CRL agent to learn a previously unseen task. Other issues with current LL techniques are further explored in section XXXXX\textbf{2.X }XXXX. 

\section{Thesis Aims and Objectives}
The primary aim of this thesis is to deliver a CRL method that can adapt to a continuously changing environment in a scalable manner. The scalability of LLRL techniques is critical if we aim to eventually achieve artificial intelligences that can react and learn when faced with as many data inputs as our own sensory inputs are - without failure. Humans intuitively understand not to walk off a ledge without having personally tried it before, and this is the sense of intuition that I hope to create within an Artificial Intelligence. 

\section{Thesis Assumptions}
Not sure what to write here.

\section{Thesis Contribution}
This thesis identifies that the field of LLRL can be complemented by terms originating in Information Theory to create more scalable techniques. Unlike existing LLRL techniques that facilitate continuous learning, the methods outlined in this thesis have minimal storage requirements, and rather focus on  creating agents that bequeath their memories of different tasks to create an inherited intuition as it were.  The method of chaining together the memories of selective LLRL agents is the primary contribution of this thesis. 

\section{Document Structure}
Following on from this introduction chapter, Chapter 2 will explain any required background knowledge on Reinforcement Learning and Information Theory methods. Chapter 3 then presents the proposed experimental theory behind this thesis including any associated reasoning. Chapter 4 will describe the experimental implementation of the theory presented in Chapter 3, and we will then evaluate the results in Chapter 5 with respect to existing LLRL techniques. The thesis concludes with Chapter 6, in which the reader may find a summary of the work and an outline of any remaining or discovered issues.