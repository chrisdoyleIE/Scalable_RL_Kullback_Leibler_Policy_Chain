\chapter{Background and Related Work}

This section covers the knowledge necessary to understand how the learning techniques in this thesis, and how information theory can be employed alongside these methods to facilitate continuous reinforcement learning (CRL). First we will look at Reinforcement Learning and the Policy Gradient Approaches that this thesis requires to facilitate CRL. The second second of this chapter explores how exactly information theory can measure the divergence between probability distributions, and how this relates back to CRL.

\section{Reinforcement Learning}
As discussed in Chapter 1, this thesis focusses primarily on model-free reinforcement learning (RL). Reinforcement learning is where an agent must interact with an environment in order to learn how to map situations to actions so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them [1]. The environment in question can be said to be made up of a finite number of states which are described by the set $$\mathcal{S}$$. 
